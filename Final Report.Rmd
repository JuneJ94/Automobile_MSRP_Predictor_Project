---
title: 'STAT 420: Final Project Report'
authors: "Team"
output:
  html_document:
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
---

# Introduction



# Methods



##  Installing packages (commented out) and loading libraries for future use.
```{r warning=FALSE}
#install.packages("mltools")
#install.packages("caret")
#install.packages("dplyr")
#install.packages("ggplots2")
#install.packages('DT')
library(mltools)
library(data.table)
library(knitr)
library(dplyr)
library(ggplot2)
library(caret)
library(faraway)
library(lmtest)
library(MASS)
library(DT)

```

## Data Processing

**Loading dataset in R from the CSV file, and removing some columns which are not needed**
```{r}
car_data = read.csv("Cars_data.csv")
car_data = subset(car_data, select = -c(Vehicle.Style, Market.Category))
car_data = na.omit(car_data)

unique(car_data$Make)
#colnames(car_data)
#unique(car_data$Engine.Fuel.Type)
#unique(car_data$Driven_Wheels)
#unique(car_data$Vehicle.Size)
car_data$Model<- NULL
#car_data_info = car_data
#head(car_data)
colnames(car_data)

```


## Dataset Overview:

**The dataset car_data now contains the following variables:**

**Make** - The car brand

**Year** - The year the car model was released

**Engine.Fuel.Type** - The Fuel which the car runs on

**Engine.HP** - The standard measure of power of the car's engine

**Engine.Cylinders** - Number of Cylinders in the engine of the car

**Transmission.Type** - Specifies the transmission type, self explanatory

**Driven_Wheels** - Specifies to which wheels the car sends power

**Number.of.Doors** - Number of doors in the car - self explanatory

**Vehicle.Size** - Specifies the size of the car - compact, midsize, large

**highway.MPG** - Estimated miles per gallon the car travels on the highway

**city.mpg** - Estimated miles per gallon the car travels in the city

**Popularity** - Value assigned based on Twitter scraping - is part of the original dataset

**MSRP** - The response in our model. It stands for Manufacturer's Suggested Retail Price.




## Engineering the data

```{r}
hist( car_data$MSRP, scien = FALSE, col = "lightblue")
```


**Removing extreme prices less than $8,500 and greater than $50,000**

We decided to do this since there were certain extreme outliers as visible in the graph above. Reasons are further explained below.

```{r}

car_data_priced<-car_data[!(car_data$MSRP>50000 | car_data$MSRP< 8500 ),]
range(car_data_priced$MSRP)

```


Once this is done, the MSRP Histogram looks like:

```{r}
hist( car_data_priced$MSRP, col = "lightpink", breaks = 40)
```

This is a reasonable distribution for our response variable. Additionally, price is a much more important factor in the mass market than in the expensive/luxury segment so it is reasonable to restrict the price at $50,000. In fact, it may even be brought down further. -  https://smallbusiness.chron.com/price-sensitivity-product-65805.html



**Removing the non automatic/manual transmission types, and storing this new data in car_data_transd dataframe**

This is done for simplicity. There are very few automobiles with non automatic/manual transmissions, so our model would not be accurate at predicting these even if we retained these values.

```{r}


car_data_transd<-car_data_priced[!(car_data_priced$Transmission.Type=="AUTOMATED_MANUAL" | car_data_priced$Transmission.Type=="DIRECT_DRIVE" | car_data_priced$Transmission.Type=="UNKNOWN"),]

unique(car_data_transd$Transmission.Type)

```



**Removing certain fuel types, keeping only gasoline and diesel. Storing the result in car_data_fuel dataframe**

This is once again done for simplicity when it comes to predictors. We have a large dataset, so we can afford to omit certain types of automobiles. Additionally, since there were only few of non gasoline/diesel vehicles and we are targeting the mass market, this makes sense. We understand that the electric car segment is growing, and a more up-to-date dataset (this one spans all the way from 1990 to 2017) would help us cater to that market.

```{r}
car_data_fuel<-car_data_transd[!(grepl("flex", car_data_transd$Engine.Fuel.Type, fixed = TRUE)
|car_data_transd$Engine.Fuel.Type=="electric" | car_data_transd$Engine.Fuel.Type=="" | car_data_transd$Engine.Fuel.Type=="natural gas"),]

unique(car_data_fuel$Engine.Fuel.Type)
```


**Assigning the different types of gasoline to a single "gasoline value". Now, the only two values for fuel type will be "gasoline" and "diesel" as visible below**

Here, we combine the different types of gasoline into one.

```{r}
car_data_fuel$Engine.Fuel.Type[car_data_fuel$Engine.Fuel.Type == "premium unleaded (required)" ] <- "gasoline"
car_data_fuel$Engine.Fuel.Type[car_data_fuel$Engine.Fuel.Type == "regular unleaded" ] <- "gasoline"
car_data_fuel$Engine.Fuel.Type[car_data_fuel$Engine.Fuel.Type == "premium unleaded (recommended)" ] <- "gasoline"

unique(car_data_fuel$Engine.Fuel.Type)

```


**Removing extremely rare brands in our dataset (those which occur less than 20 times)**

```{r}
big_brands = names(which(table(car_data_fuel$Make)>20))

small_removed = filter(car_data_fuel, Make %in% big_brands)


```




#### Making categorical variables factors, and adding age variable

**The ReleasedYearsAgo added variable is essentially how many years ago the model was released. It is the "Year" in the dataset subtracted from the current year**

```{r}
car_data_factored = small_removed

#car_data_factored <- car_data_fuel[!(as.numeric(car_data_fuel$Make) %in% which(table(car_data_fuel$Make)<100)),]
car_data_factored$Vehicle.Size <- factor(car_data_factored$Vehicle.Size)
car_data_factored$Transmission.Type <- factor(car_data_factored$Transmission.Type)
car_data_factored$Engine.Fuel.Type <- factor(car_data_factored$Engine.Fuel.Type)
car_data_factored$Driven_Wheels <- factor(car_data_factored$Driven_Wheels)
car_data_factored$Engine.Cylinders <- factor(car_data_factored$Engine.Cylinders)
car_data_factored$Number.of.Doors <- factor(car_data_factored$Number.of.Doors)
car_data_factored$Make <- factor(car_data_factored$Make, exclude = FALSE)




levels(car_data_factored$Vehicle.Size)
levels(car_data_factored$Transmission.Type)
levels(car_data_factored$Engine.Fuel.Type)
levels(car_data_factored$Driven_Wheels)
levels(car_data_factored$Engine.Cylinders)
levels(car_data_factored$Number.of.Doors)
levels(car_data_factored$Make)


car_data_factored$ReleasedYearsAgo <- with(car_data_factored, 2020 - Year)

```

**Removing repetitive/unnecessary variable(s)**

```{r}
car_data_factored$Year <- NULL
```


## Modelling


**Splitting data into train and test: **

```{r}
set.seed(100)

#train-test  split using 65% of the data
samplesize = round(0.65*nrow(car_data_factored), 0)
index = sample(seq_len(nrow(car_data_factored)), size = samplesize)

data_train = car_data_factored[index,]
data_test = car_data_factored[-index,]
```


**Creating a basic additive model:**
```{r}
msrp_mod_additive = lm(MSRP ~. , data_train)
summary(msrp_mod_additive)$adj.r.sq
```

**We get a fairly high value for Adjusted R-squared, which is encouraging. We now consider some other models as well**


**Defining a function to calculate the calc_loocv_rmse **
```{r}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
```



**Creating Quadratic Model with AIC Step**
```{r}
MSRP_big_mod_poly = lm(
  MSRP ~ . + I(Engine.HP ^ 2) + I(ReleasedYearsAgo ^ 2) + I(city.mpg ^ 2) + I(highway.MPG ^ 2)  + I(Popularity ^ 2), 
  data = data_train)

MSRP_mod_both_aic_poly = step(MSRP_big_mod_poly, direction = "both", trace = 0)

```



**Creating Linear Model with AIC Step**
```{r}


#hipcenter_mod_both_aic = step(
  #hipcenter_mod_start, 
  #scope = hipcenter ~ Age + Weight + HtShoes + Ht + Seated + Arm + Thigh + Leg, 
  #direction = "both")


MSRP_big_mod_linear = lm(
  MSRP ~ . , 
  data = data_train)

MSRP_mod_both_aic_lin = step(MSRP_big_mod_linear, direction = "both", trace = 0)

summary(MSRP_mod_both_aic_lin)$r.sq
```



**Creating Linear Model with BIC Step**

```{r}
msrp_mod_start = lm(MSRP ~ ., data = data_train)
n = length(resid(msrp_mod_start))
msrp_mod_linear_both_bic = step(msrp_mod_start, direction = "both", k = log(n), trace = 0)
```


**Creating Quadratic Model with BIC Step**

```{r}
msrp_mod_start2 = lm(MSRP ~ . + I(Engine.HP^2)  + I(city.mpg^2) + I(highway.MPG^2) + I(Popularity^2) - I( ReleasedYearsAgo^2), data = data_train)
n = length(resid(msrp_mod_start2))
msrp_mod_poly_both_bic = step( msrp_mod_start2, direction = "both", k = log(n), trace = 0)
```



**Metric Table: **

```{r}

#this table compares the calc_loocv_rmse, adjusted r squared, and r squared

to_insert_1 = c("msrp_mod_additive", calc_loocv_rmse(msrp_mod_additive), 
                summary(msrp_mod_additive)$adj.r.sq, summary(msrp_mod_additive)$r.sq)

to_insert_2 = c("MSRP_mod_both_aic_lin", calc_loocv_rmse(MSRP_mod_both_aic_lin) ,summary(MSRP_mod_both_aic_lin)$adj.r.sq,summary(MSRP_mod_both_aic_lin)$r.sq )

to_insert_3 = c("MSRP_mod_both_aic_poly", calc_loocv_rmse(MSRP_mod_both_aic_poly) ,summary(MSRP_mod_both_aic_poly)$adj.r.sq,summary(MSRP_mod_both_aic_poly)$r.sq )


to_insert_4 = c("msrp_mod_linear_both_bic", calc_loocv_rmse(msrp_mod_linear_both_bic) ,summary(msrp_mod_linear_both_bic)$adj.r.sq,summary(msrp_mod_linear_both_bic)$r.sq )

to_insert_5 = c("msrp_mod_poly_both_bic", calc_loocv_rmse(msrp_mod_poly_both_bic) ,summary(msrp_mod_poly_both_bic)$adj.r.sq,summary(msrp_mod_poly_both_bic)$r.sq )


dataframe.values = c(to_insert_1, to_insert_2, to_insert_3, to_insert_4, to_insert_5)
dataframe = matrix(dataframe.values,nrow=5 ,byrow = T)
colnames(dataframe) = c("Model Name","calc_loocv_rmse","Adj. R-Sq.", "R-Sq.")

datatable(dataframe)

```

**Listing the composition of all the models**

```{r}
msrp_mod_additive$call
```


```{r}
MSRP_mod_both_aic_lin$call
```


```{r}
MSRP_mod_both_aic_poly$call
```


```{r}
msrp_mod_linear_both_bic$call

```

```{r}
msrp_mod_poly_both_bic$call
```


**We see that these are all fairly similar models, and the two which are somewhat different are MSRP_mod_both_aic_poly and msrp_mod_poly_both_bic These two consistently give marginally better values for the metrics. We choose to pursue the model MSRP_mod_both_aic_poly for now since it uses an extra variable and has a very slightly better standing metric-wise.**



##Assumptions


**Creating a few functions to evaluate the model assumptions: **


```{r}
plot_func = function(model, pointcol = "slateblue3",linecol = "limegreen") {
  plot(fitted(model), resid(model), col = pointcol, pch = 20, xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 4)

}
```

```{r}

assumption_tester = function(model) {
  
  qqnorm(resid(model), main = "Normal Q-Q Plot", col = "darkgrey")
  qqline(resid(model), col = "dodgerblue", lwd = 2)
  
  #normality test
  print("Shapiro test:")
  print(shapiro.test(resid(model)[0:5000]))
  hist(model$resid, col = "skyblue3")
  

  #multicollinearity
  vif_vals = vif(model)
  print("Max VIF Value:")
  #print(max(vif_vals))
  print(vif_vals[vif_vals > 5])
  
  #Constant Variance
  plot_func(model)
  
  print("Breusch-Pagan test:")
  print(bptest(model))
  
  print("Adjusted R-Squared:")
  print(summary(model)$r.sq)
  
  
}

```


```{r}
assumption_tester(MSRP_mod_both_aic_poly)
```

**The Shapiro-Wilk test and the qq plot indicate non-normality of errors**

**The constant variance assumption (based on the fitted vs residuals graph and the BP-test) also does appear to be violated **

**We can see that the VIF values for quite a few of the predictors are fairly high if we go with a threshold of 5. There does appear to be high multicollinearity **

```{r}
pairs(data_train, col = "dodgerblue")
```



```{r}
#Renaming the model for convenience:
chosen_model = MSRP_mod_both_aic_poly
```



## Improving the Model


### Influential Values

We can consider removing influential points from our model

```{r}
cd_chosen_mod = cooks.distance(chosen_model)
length(cd_chosen_mod[cd_chosen_mod > 4 / length(cd_chosen_mod) ])

cox_sub = cd_chosen_mod <= 4/length(cd_chosen_mod)
```

**Of the 4556 points in our chosen model, 254 appear to be influential. We can consider removing these from our model**


**We reload the model, with the influential points removed: **


```{r}
chosen_model_uninf = lm(formula = MSRP ~ Make + Engine.Fuel.Type + Engine.HP + Engine.Cylinders + 
    Transmission.Type + Driven_Wheels + Number.of.Doors + Vehicle.Size + 
    highway.MPG + ReleasedYearsAgo + I(Engine.HP^2) + I(ReleasedYearsAgo^2) + 
    I(city.mpg^2) + I(highway.MPG^2), data = data_train, subset = cox_sub)

summary(chosen_model_uninf)$adj.r.sq
```

**We can see that this removal of influential points has increased the adjusted R-squared from 0.84 to 0.88, which is expected and encouraging.**



### Satisfying Assumptions


#### Normality Assumption


**We can consider the Box-Cox Transformation method since our response variable (MSRP) is strictly positive**

```{r}
boxcox(chosen_model_uninf, plotit = TRUE, lambda = seq(0.3, 0.5, by = 0.1))
```


**From this, we see that λ = 0.25 is extremely close to the maximum and within the confidence interval.**


**We can now fit a model with the transformation of λ = 0.25 applied to the response variable: **


```{r}


chosen_model_trans1 = lm(formula = (((MSRP ^ 0.37) - 1) / 0.37)  ~ Make + Engine.Fuel.Type + Engine.HP + Engine.Cylinders + 
    Transmission.Type + Driven_Wheels + Number.of.Doors + Vehicle.Size + 
    highway.MPG + ReleasedYearsAgo + I(Engine.HP^2) + I(ReleasedYearsAgo^2) + 
    I(city.mpg^2) + I(highway.MPG^2), data = data_train, subset = cox_sub)

assumption_tester(chosen_model_trans1)


```

**Since our λ is somewhat close to 0, we  also consider a log transformation on the dependant variable:**

```{r}

chosen_model_trans2 = lm(formula = log(MSRP) ~ Make + Engine.Fuel.Type + Engine.HP + Engine.Cylinders + 
    Transmission.Type + Driven_Wheels + Number.of.Doors + Vehicle.Size + 
    highway.MPG + ReleasedYearsAgo + I(Engine.HP^2) + I(ReleasedYearsAgo^2) + 
    I(city.mpg^2) + I(highway.MPG^2), data = data_train, subset = cox_sub)

assumption_tester(chosen_model_trans2)

```

**Using the log transformation, we have satisfied the normality assumption.**

**We can see that we have improved the p-value of the Shapiro-Wilk normality test from less than < 2.2e-16 to a value of roughly 0.2. This means we fail to reject the null hypothesis, which is what we want. This is a substantial improvement. **

**Additionally, the histogram of the residuals and the Q-Q Plot are much better than before, both indicating the same progress.**

**While the adjusted R-Squared for the log transformed model is lower, we still prefer this model since it satisfies the normality assumption**


### Collinearity

```{r}
summary(chosen_model_trans2)
```


**As we saw from the VIF results, there are a few predictors which show signs of multicollinearity. We can start by removing the predictor Engine.Cylinders, since all the dummy variables associated to it have high VIFs and their p-values are also fairly high (from the summary).**



```{r}
chosen_model_drop1 = lm(formula = log(MSRP) ~ Make + Engine.Fuel.Type + Engine.HP +
    Transmission.Type + Driven_Wheels + Number.of.Doors + 
    Vehicle.Size + highway.MPG + ReleasedYearsAgo + I(Engine.HP^2) + 
    I(ReleasedYearsAgo^2) + I(city.mpg^2) + I(highway.MPG^2), 
    data = data_train, subset = cox_sub)


vif(chosen_model_drop1)[vif(chosen_model_drop1) > 5]
summary(chosen_model_drop1)
```



```{r}
assumption_tester(chosen_model_drop1)
```

**We choose to keep the remaining predictors with high VIF values. We make this decision since all the predictors with high VIFs are caused due to the inclusion of polynomial predictors of the same variables. All the predictors with high VIF vales are significant to the model (as visible in the summary) and it makes sense to retain them.**



# Results

## Testing 


**Retrieving the predictions made by our model on the test data**
```{r}
test_predictions_log = predict(chosen_model_drop1, newdata = data_test, type = "resp")

test_predictions = exp(test_predictions_log)

```


**Plotting the test-results:**

```{r}

data = data.frame(
  x= test_predictions,
  y= data_test$MSRP
)


plot(data$x, data$y,
     pch=1, 
     cex=1, 
     col="paleturquoise3",
     xlab="Predicted Value of MSRP", ylab="Actual Value of MSRP",
     main="Predicted vs Actual MSRP"
     )

abline(0,1, col="navyblue",  lwd = 2)

```


**Since we are plotting the predicted vs the actual MSRPs of the test data, we expect the chart to be oriented at 45 degrees positively, as depicted by the dark blue line. Other than one outlier ~(65000, 47000), our predictions are fairly closely matched to the actual values of the MSRP**
