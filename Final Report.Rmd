---
title: 'STAT 420: Final Project Report'
authors: "Team"
output:
  html_document:
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
---



##  Installing packages (commented out) and loading libraries for future use.
```{r}
#install.packages("mltools")
#install.packages("caret")
#install.packages("dplyr")
#install.packages("ggplots2")
library(mltools)
library(data.table)
library(knitr)
library(dplyr)
library(ggplot2)
library(caret)
library(faraway)
library(lmtest)
library(MASS)

```


**Loading dataset in R from the CSV file, and removing some columns which are not needed**
```{r}
car_data = read.csv("Cars_data.csv")
car_data = subset(car_data, select = -c(Vehicle.Style, Market.Category))
car_data = na.omit(car_data)

#unique(car_data$Transmission.Type)
#colnames(car_data)
#unique(car_data$Engine.Fuel.Type)
#unique(car_data$Driven_Wheels)
#unique(car_data$Vehicle.Size)
#car_data$Make<-NULL
car_data$Model<- NULL
car_data_info = car_data
#head(car_data)

```


## Dataset Overview:

**The dataset car_data now contains the following variables:**

xx
xx
xx
xx













## Engineering the data

```{r}
hist( car_data$MSRP, scien = FALSE, col = "lightblue")
```


**Removing extreme prices less than $10,000 and greater than $100,000**

We decided to do this since there were certain extreme outliers as visible in the graph above. 
```{r}

car_data_priced<-car_data[!(car_data$MSRP>100000 | car_data$MSRP< 10000 ),]
range(car_data_priced$MSRP)

```


Once this is done, the MSRP Histogram looks like:

```{r}
hist( car_data_priced$MSRP, scientific = FALSE, col = "lightpink")
```

**This is a reasonable distribution for our response variable. Additionally, price is a much more important factor in the expensive/luxury segment than in the mass market -  https://smallbusiness.chron.com/price-sensitivity-product-65805.html**



**Removing the non automatic/manual transmission types, and storing this new data in car_data_transd dataframe**

This is done for simplicity. There are very few automobiles with non automatic/manual transmissions, so our model would not be accurate at predicting these.

```{r}


car_data_transd<-car_data_priced[!(car_data_priced$Transmission.Type=="AUTOMATED_MANUAL" | car_data_priced$Transmission.Type=="DIRECT_DRIVE" | car_data_priced$Transmission.Type=="UNKNOWN"),]

unique(car_data_transd$Transmission.Type)

```



**Removing certain fuel types, keeping only gasoline and diesel. Storing the result in car_data_fuel dataframe**

This is done for simplicity when it came to predictors. We have a large dataset, so we can afford to omit certain types of automobiles. Additionally, since there were only few of non gasoline/diesel vehicles and we are targeting the mass market, this makes sense. We understand that the electric car segment is growing, and a more up-to-date dataset (this one spans all the way from 1990 to 2017) would help us cater to that market.

```{r}
car_data_fuel<-car_data_transd[!(grepl("flex", car_data_transd$Engine.Fuel.Type, fixed = TRUE)
|car_data_transd$Engine.Fuel.Type=="electric" | car_data_transd$Engine.Fuel.Type=="" | car_data_transd$Engine.Fuel.Type=="natural gas"),]

unique(car_data_fuel$Engine.Fuel.Type)
```


**Assigning the different types of gasoline to a single "gasoline value". Now, the only two values for fuel type will be "gasoline" and "diesel" as visible below**

Here, we combine the different types of gasoline into one.

```{r}
car_data_fuel$Engine.Fuel.Type[car_data_fuel$Engine.Fuel.Type == "premium unleaded (required)" ] <- "gasoline"
car_data_fuel$Engine.Fuel.Type[car_data_fuel$Engine.Fuel.Type == "regular unleaded" ] <- "gasoline"
car_data_fuel$Engine.Fuel.Type[car_data_fuel$Engine.Fuel.Type == "premium unleaded (recommended)" ] <- "gasoline"

unique(car_data_fuel$Engine.Fuel.Type)

```



**Making categorical variables factors, and adding age variable**

**The agevariable is essentially how many years ago the model was released. It is the "Year" in the dataset subtracted from the current year**
```{r}
car_data_factored = car_data_fuel
car_data_factored$Vehicle.Size <- factor(car_data_factored$Vehicle.Size)
car_data_factored$Transmission.Type <- factor(car_data_factored$Transmission.Type)
car_data_factored$Engine.Fuel.Type <- factor(car_data_factored$Engine.Fuel.Type)
car_data_factored$Driven_Wheels <- factor(car_data_factored$Driven_Wheels)
car_data_factored$Engine.Cylinders <- factor(car_data_factored$Engine.Cylinders)
car_data_factored$Number.of.Doors <- factor(car_data_factored$Number.of.Doors)
car_data_factored$Make <- factor(car_data_factored$Make)

levels(car_data_factored$Vehicle.Size)
levels(car_data_factored$Transmission.Type)
levels(car_data_factored$Engine.Fuel.Type)
levels(car_data_factored$Driven_Wheels)
levels(car_data_factored$Engine.Cylinders)
levels(car_data_factored$Number.of.Doors)
levels(car_data_factored$Make)


#car_data_factored = one_hot(as.data.table(car_data_factored))

car_data_factored$ReleasedYearsAgo <- with(car_data_factored, 2020 - Year)

```

**Removing repetitive/unnecessary variable(s)**

```{r}
car_data_factored$Year <- NULL
```


## Modeling


**Simple additive models:**

```{r}
set.seed(100)
#train-test  split using 65% of the data
samplesize = round(0.65*nrow(car_data_factored), 0)
index = sample(seq_len(nrow(car_data_factored)), size = samplesize)
data_train = car_data_factored[index,]
data_test = car_data_factored[-index,]
msrp_mod = lm(MSRP ~. - Popularity, data_train)
summary(msrp_mod)
msrp_mod2 = lm(MSRP ~ highway.MPG + Popularity, data_test)
#summary(msrp_mod2)
#anova(msrp_mod2, msrp_mod)
```
**We get a fairly high value for Adjusted R. Squared, which is encouraging. We can now dig deeper and look at assumption violations and potential improvements in the model.**



```{r}
alias(msrp_mod)
```


**Assumptions**


```{r}
plot_func = function(model, pointcol = "blue",linecol = "green") {
  plot(fitted(model), resid(model), col = pointcol, pch = 20, xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 2)

}
```

```{r}

assumption_tester = function(model) {
  
  qqnorm(resid(model), main = "Normal Q-Q Plot", col = "darkgrey")
  qqline(resid(model), col = "dodgerblue", lwd = 2)
  
  #normality test
  print("Shapiro test:")
  print(shapiro.test(model$residuals[0:5000]))
  hist(model$resid)
  

  #multicollinearity
  vif_vals = vif(model)
  print("Max VIF Value:")
  print(max(vif_vals))
  print(vif_vals[vif_vals > 10])
  
  #Constant Variance
  plot_func(model)
  
  print("Breusch-Pagan test:")
  print(bptest(model))
  
  print("Adjusted R-Squared:")
  print(summary(model)$r.sq)
  
  
}

```



```{r}
assumption_tester(msrp_mod)
```


**Here, we can see that the VIF values for Engine.Cylinders and highway.MPG are fairly high if we go with a threshold of 10. There does appear to be some multicollinearity **

**The Shapiro-Wilk test and the qq plot also indicate non-normality**

**The constant variance assumption (based on the fitted vs residuals graph and the BP-test) also does appear to be violated **



## Managing Assumption Violations

## #Multicollinearity

**We can look into removing the Engine.Cylinders variable and seeing how this impacts the model**

```{r}
msrp_mod_no_cyl = lm(MSRP ~. - Popularity - Engine.Cylinders, data_train)
vif(msrp_mod_no_cyl)[vif(msrp_mod_no_cyl) > 10]
```

**We can further considering removing highway.MPG , since this issue is probably being caused by the highway.MPG and city.mpg being closely related**

```{r}
msrp_mod_no_cyl_highmpg = lm(MSRP ~. - Popularity - Engine.Cylinders - highway.MPG, data_train)
vif(msrp_mod_no_cyl_highmpg)[vif(msrp_mod_no_cyl_highmpg) > 5]

```

**Now, we can see that none of the VIFs are more than 5, which is well below the threshold.**



```{r}
assumption_tester(msrp_mod_no_cyl_highmpg)
```
**We see that we have given up on some prediction accuracy in order to satisfy the model assumption**



## Influential Points:

We can consider removing influential points from our model

```{r}
msrp_mod_no_cyl_highmpg_cd = cooks.distance(msrp_mod_no_cyl_highmpg)
msrp_mod_influential_fix = lm(MSRP ~. - Popularity - Engine.Cylinders - highway.MPG, data = data_train, 
                              subset = msrp_mod_no_cyl_highmpg_cd <= 4/length(msrp_mod_no_cyl_highmpg_cd))

assumption_tester(msrp_mod_influential_fix)
```

**This has marginally helped our value of adjusted R-squared, which is once more encouraging**


**In order to work towards the normality assumption and the multicollinearity, we will need to consider transformations on our predictors and our response variables**


## Transformations

**Since our response variable (MSRP) is strictly positive, we can consider Box-Cox transformations**
```{r}

```
















**Trying Polynomial Model with AIC choice**
```{r}
MSRP_big_mod = lm(
  MSRP ~ . + I(Engine.HP ^ 2) + I(ReleasedYearsAgo ^ 2) + I(city.mpg ^ 2) + I(highway.MPG ^ 2)  + I(Popularity ^ 2), 
  data = data_train)

MSRP_mod_back_aic = step(MSRP_big_mod, direction = "backward", trace = 0)

summary(MSRP_mod_back_aic)

```





```{r}
#assumption_tester(MSRP_big_mod)
```


```{r}
#alias(MSRP_mod_back_aic)
#assumption_tester(MSRP_mod_back_aic)
```



**Making model improvements**

```{r}
car_removed_predictors = lm(log(MSRP) ~ Make + Engine.Fuel.Type + log(Engine.HP) +  Transmission.Type + 
    Driven_Wheels + Number.of.Doors + 
    I(ReleasedYearsAgo^2) + I(city.mpg^2) + 
    I(highway.MPG^2) , data = data_train)
```

```{r}
assumption_tester(car_removed_predictors)
```

```{r}
alias(car_removed_predictors)
#summary(car_removed_predictors)
```






```{r}
#plot( MSRP~Engine.HP, data = car_data_factored, scientific = FALSE)

#hist( car_data_factored$MSRP, scientific = FALSE)

```



**This model does not **
